
###Ensemble learning: Gradient Boosting
Boosting is the upgraded version of bagging(bootstrap aggregation). Rather than training several models from scratch and aggregating the results, each new model is trained based on error from the previous model. In other words, new models introduced is in turned trained by learning the mistakes of previous models. All models are then aggregated to form the final result. 

A popular algortihm that uses this technique is XGBOOST:

```{r}
library(tidyverse)
library(tidymodels)
library(xgboost)
data <- read.csv('../../data/courseGrades.csv')
processed_data <- data %>% 
  filter(nLetterStudents != 0) %>% 
  select(course, instructor, quarter, year, nLetterStudents, dept, avgGPA) %>%
  mutate(course = str_replace_all(course, "\\s+", " "))

```

```{r}
stat_classes <- processed_data %>% 
  filter(!(quarter == "summer") & dept == "PSTAT" & between(year, 2017, 2022))

stat_classes <- stat_classes %>% select(-dept)
```


```{r}
# Install required packages if not installed
# install.packages(c("tidymodels", "xgboost"))

# Load required libraries


# Assuming 'df' is your data frame with the dataset
# 'avg_gpa' is your target variable, and other columns are features

# Step 1: Create Recipe
recipe <- recipe(avgGPA ~ ., data = stat_classes) %>%
  step_dummy(all_nominal(), one_hot = TRUE)

# Step 2: Create Model Specification
model_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

# Step 3: Create Workflow
workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(model_spec)

# Step 4: Split Data
data_split <- initial_split(stat_classes, prop = 0.8, strata = instructor)
train_data <- training(data_split)
test_data <- testing(data_split)

# Step 5: Train and Evaluate Model
model <- workflow %>%
  fit(train_data)

# Step 6: Predictions and Evaluation
predictions <- predict(model, test_data) %>%
  bind_cols(test_data)
mse <- mean((predictions$avgGPA - predictions$.pred)^2)
cat("Mean Squared Error:", mse, "\n")


```
```{r}
mse <- mean((predictions$avgGPA - predictions$.pred)^2)
mae <- mean(abs(predictions$avgGPA - predictions$.pred))

cat("Mean Squared Error:", mse, "\n")
cat("Mean Absolute Error:", mae, "\n")

# Additional Step: Model Accuracy (Optional)
# For a regression model, accuracy might not be the most suitable metric,
# but you can calculate the proportion of predictions within a certain threshold
threshold <- 0.25  # Example threshold, adjust as needed
accuracy <- mean(abs(predictions$avgGPA - predictions$.pred) <= threshold)
cat("Accuracy (within threshold):", accuracy, "\n")
```


```{r}
library(vip)


# Step 7: Model Interpretability
# Extract feature importance using the vip package
feature_importance <- vip(xgb_model)
print("Feature Importance:")
print(feature_importance)

```


##Connection to deep learning
Like the name suggest, gradient boosting is analogous to gradient descent central to deep learning algorithms. Where as gradient descent tries to minimize the loss function by adjusting its parameters, gradient boosting minimize loss function by adding new models.






