---
title: "Bootstrap Introduction"
author: "Sharanya Sharma"
date: "2023-12-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Bootstrap Introduction

In an ideal scenario, statisticians will have an adequate amount of data that can then be divided into training data, validation data, and test data in order to make the most accurate predictions. However, there are many situations in which there is not a sufficient amount of data that can be divided into these three categories. In such cases, certain re-sampling techniques can be used to provide sufficient data. One such re-sampling technique is called bootstrap. In bootstrapping, a large number of subsets of equal size are taken from the original dataset. Each of the selected subsets are chosen through random sampling, without replacement. The observations not selected are called Out-Of-Bag, which can be used to estimate test error without the use of cross validation. Approximately 1/3 of the observations tend to be Out-of-Bag. From the subsets that are chosen, the same statistical method is then fit on each of these subsets. For regression purposes, the mini models are combined by averaging the results, whereas in classification the mode is taken. 

## Bootstrap Diagram

```{r}
library(imager)
bootstrap_diagram <- load.image("/Users/sharanya/Documents/GitHub/vignette-bootstrapping/images/bootstrap-diagram.png")
plot(bootstrap_diagram, axes=FALSE)

```
In the above example, the dataset only has 3 observations, thus bootstrapping should be used to create a sufficient amount of data for regression. The total dataset of 3 observations is sampled 3 times without replacement to acquire the first bootstrap sample. This process is repeated "B" times to acquire B total bootstrap samples. Each sample is then fit to a statistical model, resulting in B number of models: $\hat{f}^{*1}... \hat{f}^{*B}$. The mini models are then aggregated to form one model. This bootstrap aggregating method is known as Bagging.


Image Reference: Yu, Guo. "Lecture 8: Cross-Validation & Bootstrap", PSTAT-131/231: Introduction to Statistical Machine Learning, Oct.26, 2023, UC Santa Barbara.


## Bootstrap Sampling Technique

```{r}
# store each bootstrap iteration in a list
sample_list <- list()

# n=1000 bootstrap samples
for(i in 1:10000){
   sample_list[[i]] <- sample(c(1:10000), size = 10000, replace = TRUE)
}

# head(sample_list)

```

The head function prints the first 6 bootstrap samples. We can see that each of these bootstrap samples include 10000 randomly sampled numbers without replacement from the original 1000 observations. The entire sample_list is populated with 10000 samples, where each sample holds 10000 observations.


## Out-Of-Bag Probability

```{r}
# select a random number to test ratio of missing observations
random_num <- sample(c(1:10000), size = 1, replace = TRUE)

# calculating out-of-bag probability 
count <- 0
for(i in 1:10000){
  if(random_num %in% unique(sample_list[[i]])){
    count = count + 1
  }
}
probability <- 1-(count/10000.0)

probability
```

Run this code chunk multiple times to see how the probability changes. We can see that each time the probability is close to 1/3.



